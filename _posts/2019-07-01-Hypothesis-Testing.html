--- 
layout: post 
title: Hypothesis Testing
comments: true 
---


<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>'HypothesisTesting'</title>
  <style>
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <style>
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<div class="cell markdown">
<p>Today, we are going to see 4 ways of Hyptothesis testing approaches.</p>
</div>
<div class="cell markdown">
<p><strong>Background</strong></p>
<p>A Survey investigates whether a pregnant woman should obtain a legal abortion if the woman wants it for any reason. 895/1934 reported Yes and the rest reported No.</p>
<p>Let <span class="math inline">\(\theta\)</span> denote the unknown population proportion of respondents who are for the proposition. The question of interest is whether a majority of the population supports the proposition in the survey item.</p>
<p><em>Frequentist approach.</em></p>
<p>The estimate of <span class="math inline">\(\theta\)</span> is <span class="math inline">\(\hat{\theta}\)</span> = 895/1934 = .46. The data is binomial (independent Bernoulli trials) in Nature. However, given large sample, we can use normal distribution to approximate the frequentist sampling distribution of <span class="math inline">\(\hat{\theta}\)</span>.</p>
<p>Typically, we test the null hypothesis <span class="math inline">\(H_0\)</span> : <span class="math inline">\(\theta = .5\)</span> against all other alternatives <span class="math inline">\(H_A\)</span> : <span class="math inline">\(\theta \ne .5\)</span>, or against a one-sided alternative <span class="math inline">\(H_B\)</span> : <span class="math inline">\(\theta &gt;.5\)</span>.</p>
<p>We then ask how unlikely it is that the value of <span class="math inline">\(\hat{\theta}\)</span> actually obtained by centering the sampling distribution of <span class="math inline">\(\hat{\theta}\)</span> at the hypothesized value (under <span class="math inline">\(H_0\)</span>). The standard deviation of the normal sampling distribution (or the standard error of <span class="math inline">\(\hat{\theta}\)</span>) under <span class="math inline">\(H_0\)</span> is</p>
<p><span class="math display">\[
se(\hat{\theta}) = \sqrt{\frac{\theta_{H_0}(1 - \theta_{H_0})}{n}} = 
\sqrt{\frac{0.5 (1 - 0.5)}{1934}} \approx 0.011
\]</span></p>
<p>The realized value of <span class="math inline">\(\theta\)</span> is (.5 − .46)/.011 <span class="math inline">\(\approx\)</span> 3.64 standard errors away from the hypothesized value. The corresponding proportion (two-sided p-value) is</p>
<p><span class="math display">\[2\times\int_{3.64}^{\infty}\phi(z)dz=2\times[1-\Phi(3.64)]\approx0.00028\]</span></p>
<p>where <span class="math inline">\(\phi\)</span> and <span class="math inline">\(\Phi\)</span> are respectively the pdf and cdf of normal distribution. Then, frequentists would conclude that the null hypothesis should be rejected with a p-values for <span class="math inline">\(H_0\)</span> against <span class="math inline">\(H_A\)</span> as .00028, and against <span class="math inline">\(H_B\)</span> as .00014.</p>
<p>That means, over <strong>repeated</strong> random samplings, a large proportion of estimates of <span class="math inline">\(\hat{\theta}\)</span> will lie 3.64 or more standard errors away from the hypothesized mean of the sampling distribution. This is an extremely rare event, under a normal distribution.</p>
<p><em>Bayesian Approach</em> ...... Basics</p>
<p>With out loss of generality, assume <span class="math inline">\(\theta\)</span> follows uniform distribution. Then, the posterior has the same shape as the likelihood. With large sample, the likelihood is approximated by a normal density with <span class="math inline">\(p(\theta|y) \approx N(.46,.011)\)</span>. Centered at 0.46, most of the posterior probability mass lies below .5, suggesting that the hypothesis θ &gt;.5 be not well-supported by the data.</p>
<p><span class="math display">\[
\text{Pr}(\theta&gt;0.5|y)=\int_{0.5}^{\infty}p(\theta|y)d\theta=\int_{0.5}^{\infty}\phi(\frac{\theta-0.46}{0.011})dz\approx0.00014
\]</span></p>
<p>The Bayesian probability relies on the researcher’s beliefs about <span class="math inline">\(\theta\)</span>, updated via application of Bayes Rule, with the posterior distribution being Pr<span class="math inline">\((H_0|y)\)</span>.</p>
<p>The frequentist p-value is obtained differently and has a different interpretation. It conditions on the null hypothesis that the sampling distribution is <span class="math inline">\(f( \hat{\theta}|H_0)\)</span>. The p-value for <span class="math inline">\(H_0\)</span> against the alternatives are obtained under repeated sampling.</p>
<p>The code implementation is as follows.</p>
</div>
<div class="cell code" data-execution_count="2" data-trusted="true">
<div class="sourceCode" id="cb1"><pre class="sourceCode R"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1"></a><span class="co">## Code implementation</span></span>
<span id="cb1-2"><a href="#cb1-2"></a></span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="kw">library</span>(ggplot2)</span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="kw">library</span>(magrittr)</span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="kw">library</span>(gridExtra)</span>
<span id="cb1-6"><a href="#cb1-6"></a></span>
<span id="cb1-7"><a href="#cb1-7"></a>graphs &lt;-<span class="st"> </span><span class="kw">new.env</span>()</span>
<span id="cb1-8"><a href="#cb1-8"></a></span>
<span id="cb1-9"><a href="#cb1-9"></a>graphs<span class="op">$</span>draw_posterior &lt;-<span class="st"> </span><span class="cf">function</span>(<span class="dt">data =</span> <span class="kw">c</span>(<span class="dv">895</span>, <span class="dv">1039</span>)) {</span>
<span id="cb1-10"><a href="#cb1-10"></a>    h0 &lt;-<span class="st"> </span><span class="fl">0.5</span></span>
<span id="cb1-11"><a href="#cb1-11"></a>    m &lt;-<span class="st"> </span>data[<span class="dv">1</span>]<span class="op">/</span><span class="kw">sum</span>(data)</span>
<span id="cb1-12"><a href="#cb1-12"></a>    sd &lt;-<span class="st"> </span>m<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>m)<span class="op">/</span><span class="kw">sum</span>(data) <span class="op">%&gt;%</span><span class="st"> </span>sqrt</span>
<span id="cb1-13"><a href="#cb1-13"></a>    </span>
<span id="cb1-14"><a href="#cb1-14"></a>    <span class="co"># cat(c(m,sd))</span></span>
<span id="cb1-15"><a href="#cb1-15"></a>    </span>
<span id="cb1-16"><a href="#cb1-16"></a>    p1 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">data.frame</span>(<span class="dt">x =</span> <span class="kw">c</span>(m<span class="dv">-4</span><span class="op">*</span>sd, m<span class="op">+</span><span class="dv">4</span><span class="op">*</span>sd)), <span class="kw">aes</span>(x)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb1-17"><a href="#cb1-17"></a><span class="st">    </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> dnorm, <span class="dt">args =</span> <span class="kw">list</span>(<span class="dt">mean =</span> m, <span class="dt">sd =</span> sd)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb1-18"><a href="#cb1-18"></a><span class="st">    </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> m, <span class="dt">linetype =</span> <span class="st">&#39;dashed&#39;</span>, <span class="dt">color =</span> <span class="st">&#39;red&#39;</span>) <span class="op">+</span></span>
<span id="cb1-19"><a href="#cb1-19"></a><span class="st">    </span><span class="kw">ggtitle</span>(<span class="st">&#39;Posterior density&#39;</span>)</span>
<span id="cb1-20"><a href="#cb1-20"></a>                  </span>
<span id="cb1-21"><a href="#cb1-21"></a>    p2 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">data.frame</span>(<span class="dt">x =</span> <span class="kw">c</span>(<span class="fl">0.495</span>, <span class="fl">0.51</span>)), <span class="kw">aes</span>(x)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb1-22"><a href="#cb1-22"></a><span class="st">    </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> dnorm, <span class="dt">args =</span> <span class="kw">list</span>(<span class="dt">mean =</span> m, <span class="dt">sd =</span> sd)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb1-23"><a href="#cb1-23"></a><span class="st">    </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> dnorm, <span class="dt">xlim =</span> <span class="kw">c</span>(h0, <span class="fl">0.510</span>),</span>
<span id="cb1-24"><a href="#cb1-24"></a>                  <span class="dt">args =</span> <span class="kw">list</span>(<span class="dt">mean =</span> m, <span class="dt">sd =</span> sd), <span class="dt">geom =</span> <span class="st">&quot;area&quot;</span>) <span class="op">+</span><span class="st"> </span></span>
<span id="cb1-25"><a href="#cb1-25"></a><span class="st">    </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> h0, <span class="dt">linetype =</span> <span class="st">&#39;dashed&#39;</span>, <span class="dt">color =</span> <span class="st">&#39;blue&#39;</span>) <span class="op">+</span></span>
<span id="cb1-26"><a href="#cb1-26"></a><span class="st">    </span><span class="kw">coord_cartesian</span>(<span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="fl">0.0000003</span>))</span>
<span id="cb1-27"><a href="#cb1-27"></a>                  </span>
<span id="cb1-28"><a href="#cb1-28"></a>    <span class="kw">grid.arrange</span>(p1, p2, <span class="dt">nrow =</span> <span class="dv">1</span>)</span>
<span id="cb1-29"><a href="#cb1-29"></a></span>
<span id="cb1-30"><a href="#cb1-30"></a>}</span>
<span id="cb1-31"><a href="#cb1-31"></a></span>
<span id="cb1-32"><a href="#cb1-32"></a>graphs<span class="op">$</span><span class="kw">draw_posterior</span>()</span></code></pre></div>
<div class="output display_data">
<p><img src="7a9fc9930f164f1a4ca1cbaae8530a4263dfa7c7.png" /></p>
</div>
</div>
<div class="cell code" data-execution_count="3" data-trusted="true">
<div class="sourceCode" id="cb2"><pre class="sourceCode R"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1"></a>graphs<span class="op">$</span>draw_sampling &lt;-<span class="st"> </span><span class="cf">function</span>(<span class="dt">data =</span> <span class="kw">c</span>(<span class="dv">895</span>, <span class="dv">1039</span>)) {</span>
<span id="cb2-2"><a href="#cb2-2"></a>    h0 &lt;-<span class="st"> </span><span class="fl">0.5</span></span>
<span id="cb2-3"><a href="#cb2-3"></a>    m &lt;-<span class="st"> </span>data[<span class="dv">1</span>]<span class="op">/</span><span class="kw">sum</span>(data)</span>
<span id="cb2-4"><a href="#cb2-4"></a>    sd &lt;-<span class="st"> </span>m<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>m)<span class="op">/</span><span class="kw">sum</span>(data) <span class="op">%&gt;%</span><span class="st"> </span>sqrt</span>
<span id="cb2-5"><a href="#cb2-5"></a>    </span>
<span id="cb2-6"><a href="#cb2-6"></a>    <span class="co"># cat(c(m,sd))</span></span>
<span id="cb2-7"><a href="#cb2-7"></a>    </span>
<span id="cb2-8"><a href="#cb2-8"></a>    p1 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">data.frame</span>(<span class="dt">x =</span> <span class="kw">c</span>(h0<span class="dv">-4</span><span class="op">*</span>sd, h0<span class="op">+</span><span class="dv">4</span><span class="op">*</span>sd)), <span class="kw">aes</span>(x)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb2-9"><a href="#cb2-9"></a><span class="st">    </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> dnorm, <span class="dt">args =</span> <span class="kw">list</span>(<span class="dt">mean =</span> h0, <span class="dt">sd =</span> sd)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb2-10"><a href="#cb2-10"></a><span class="st">    </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> h0, <span class="dt">linetype =</span> <span class="st">&#39;dashed&#39;</span>, <span class="dt">color =</span> <span class="st">&#39;red&#39;</span>) <span class="op">+</span></span>
<span id="cb2-11"><a href="#cb2-11"></a><span class="st">    </span><span class="kw">ggtitle</span>(<span class="st">&#39;Sampling distribution under H_0&#39;</span>)</span>
<span id="cb2-12"><a href="#cb2-12"></a>                  </span>
<span id="cb2-13"><a href="#cb2-13"></a>    p2 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">data.frame</span>(<span class="dt">x =</span> <span class="kw">c</span>(<span class="fl">0.450</span>, <span class="fl">0.465</span>)), <span class="kw">aes</span>(x)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb2-14"><a href="#cb2-14"></a><span class="st">    </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> dnorm, <span class="dt">args =</span> <span class="kw">list</span>(<span class="dt">mean =</span> h0, <span class="dt">sd =</span> sd)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb2-15"><a href="#cb2-15"></a><span class="st">    </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> dnorm, <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="fl">0.450</span>, m),</span>
<span id="cb2-16"><a href="#cb2-16"></a>                  <span class="dt">args =</span> <span class="kw">list</span>(<span class="dt">mean =</span> h0, <span class="dt">sd =</span> sd), <span class="dt">geom =</span> <span class="st">&quot;area&quot;</span>) <span class="op">+</span><span class="st"> </span></span>
<span id="cb2-17"><a href="#cb2-17"></a><span class="st">    </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> m, <span class="dt">linetype =</span> <span class="st">&#39;dashed&#39;</span>, <span class="dt">color =</span> <span class="st">&#39;blue&#39;</span>) <span class="op">+</span></span>
<span id="cb2-18"><a href="#cb2-18"></a><span class="st">    </span><span class="kw">coord_cartesian</span>(<span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="fl">0.0000003</span>))</span>
<span id="cb2-19"><a href="#cb2-19"></a>                  </span>
<span id="cb2-20"><a href="#cb2-20"></a>    <span class="kw">grid.arrange</span>(p1, p2, <span class="dt">nrow =</span> <span class="dv">1</span>)</span>
<span id="cb2-21"><a href="#cb2-21"></a></span>
<span id="cb2-22"><a href="#cb2-22"></a>}</span>
<span id="cb2-23"><a href="#cb2-23"></a></span>
<span id="cb2-24"><a href="#cb2-24"></a>graphs<span class="op">$</span><span class="kw">draw_sampling</span>()</span></code></pre></div>
<div class="output display_data">
<p><img src="512ab788afa0fd706186649d040e78c30cd84772.png" /></p>
</div>
</div>
<div class="cell markdown">
<p><em>Bayesian Approach</em> ...... Model Choice.</p>
<p>Remember that Bayesian approach allows us to obtain a posterior density, <span class="math inline">\(f(\theta|y)\)</span>, not a point estimate or a binary decision about a hypothesis.</p>
<p>Now, hypothesis testing is a discrete decision problem. Our Bayesian procedures help us determine the ‘best model’ from a class of models for a given data set <span class="math inline">\(y\)</span>.</p>
<p>Consider two models,<span class="math inline">\(M={M_1,M_J }\)</span>. In the Bayesian approach, the researcher forms a prior belief as to which model is superior. The problem boils down to comparing <span class="math inline">\(P(M_1|y)\)</span> and <span class="math inline">\(P(M_2|y)\)</span>, where</p>
<p><span class="math display">\[P(M_{i}|y)=\frac{P(M_{i})l(\text{y}|M_{i})}{\sum_{j}P(M_{j})l(\text{y}|M_{j})}\]</span></p>
<p>where <span class="math inline">\(l(\text{y}|M_{i})\)</span> is the marginal likelihood with</p>
<p><span class="math display">\[l(\text{y}|M_{i})=\int_{\Theta}l(\text{y}|\theta_{i},M_{i})p(\theta_{i})d\theta_{i}\]</span></p>
<p>Now, we define the loss function to be <span class="math display">\[
Loss(M_{i},M*)=\begin{cases}
0 &amp; M_{i}=M*\\
1 &amp; M_{i}\ne M*
\end{cases}
\]</span></p>
<p>The expected loss becomes</p>
<p><span class="math display">\[
E[Loss]=P(\{M_{i}\ne M*\}|\text{y})=1-P(M_{i}|\text{y}).
\]</span></p>
<p>Thus we can choose the model with highest posterior probability.</p>
<p>Coming back to the last example.</p>
<p>With large sample, the likelihood for these data can be approximated by N(0.46,0.011). The posterior is proportional to this likelihood because of our assumption of uniform prior.</p>
<p>More precisely, consider the following hypotheses:</p>
<p><span class="math display">\[
H_{0}: 0.5\leq\theta\leq1 and H_{1}: 0\leq\theta&lt;0.5, 
\]</span></p>
<p><span class="math display">\[
p_{H_{0}}(\theta)=\begin{cases}
2 &amp; 0.5\leq\theta\leq1\\
0 &amp; \text{otherwise}
\end{cases}
\]</span> <span class="math display">\[
p_{H_{1}}(\theta)=\begin{cases}
2 &amp; 0\leq\theta&lt;0.5\\
0 &amp; \text{otherwise}
\end{cases}
\]</span> A priori, we are indifferent between P(H_{0})=P(H_{1}) or P(M_{0})=P(M_{1}).</p>
<p>Under H_{0}, the marginal likelihood is <span class="math display">\[
l(\text{y}|H_{0})=\int_{0.5}^{1}l(\text{y}|H_{0},\theta)p(\theta)d\theta=2(\Phi(\frac{1-0.46}{0.011})-\Phi(\frac{0.5-0.46}{0.011}))=0.00028
\]</span> Under H_{1}, we have <span class="math display">\[
l(\text{y}|H_{1})=\int_{0}^{0.5}l(\text{y}|H_{1},\theta)p(\theta)d\theta=2(\Phi(\frac{0.5-0.46}{0.011})-\Phi(\frac{0-0.46}{0.011}))=2
\]</span> Thus, <span class="math display">\[
P(H_{0}|\text{y})=\frac{\frac{1}{2}\times0.00028}{\frac{1}{2}\times0.00028+\frac{1}{2}\times2}=0.00014
\]</span> <span class="math display">\[
P(H_{1}|\text{y})=1-P(H_{0}|\text{y})=0.99986
\]</span> ...</p>
<p><span class="math inline">\(H_{1}\)</span> is more plausible.</p>
</div>
<div class="cell markdown">
<p><em>Bayesian Approach</em> ...... Bayes factors.</p>
<p>Bayes factor <span class="math inline">\(B_{10}\)</span> is defined as the ratio of marginal likelihood, which consists of posterior odds and prior odds:</p>
<p><span class="math display">\[
B_{10}=\frac{l(\text{y}|M_{1})}{l(\text{y}|M_{0})}=\frac{p(M_{1}|\text{y})}{p(M_{0}|\text{y})}/\frac{p(M_{1})}{p(M_{0})}
\]</span> In our context, <span class="math display">\[
B_{10}=\frac{l(\text{y}|H_{1})}{l(\text{y}|H_{0})}=\frac{2}{0.00028}
\]</span> Following the convention of Jefferys (1961) , when <span class="math inline">\(B_{10}&gt;12\)</span>, we say there is strong evidence that <span class="math inline">\(H_{1}\)</span> is superior to <span class="math inline">\(H_{0}\)</span>.</p>
<p>Remark.</p>
<p>In this Bayesian study, there is no reference to a sampling distribution where particular statistic is obtained. We made based on the data at hand, instead of on what might happen over repeated applications of random sampling.</p>
</div>
<div class="cell markdown">
<p>References:</p>
<ol>
<li><a href="https://sebastiansauer.github.io/normal_curve_ggplot2/" class="uri">https://sebastiansauer.github.io/normal_curve_ggplot2/</a></li>
<li><a href="https://stackoverflow.com/questions/33244629/filling-under-the-a-curve-with-ggplot-graphs" class="uri">https://stackoverflow.com/questions/33244629/filling-under-the-a-curve-with-ggplot-graphs</a></li>
<li><a href="https://stackoverflow.com/questions/24275918/how-can-you-limit-range-of-stat-function-plots-with-ggplot2" class="uri">https://stackoverflow.com/questions/24275918/how-can-you-limit-range-of-stat-function-plots-with-ggplot2</a></li>
<li><a href="http://environmentalcomputing.net/plotting-with-ggplot-adding-titles-and-axis-names/" class="uri">http://environmentalcomputing.net/plotting-with-ggplot-adding-titles-and-axis-names/</a></li>
<li>Simon Jackman, Bayesian Analysis for the Social Sciences.</li>
<li>Jeffreys, H. 1961. Theory of Probability</li>
</ol>
</div>
</body>
</html>
